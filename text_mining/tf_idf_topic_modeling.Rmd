---
title: "Text Mining Exploration: Practice Problem"
output: html_notebook
---


Load the packages needed for this practice set of problems:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)   # for visualization and data transformation
library(tidytext)    # for tidy text mining
library(topicmodels) # for topic modeling with LDA
```

Read the dataset for this exercise:

```{r, message=FALSE}
text_rei <- read_csv("https://github.com/reisanar/datasets/raw/master/fl_20_text.csv")
```

Take a look a random sample from the dataset:

```{r}
text_rei %>%
  sample_n(size = 10)
```

# Tokenization and `tf-idf`

**Perform tokenization using TF-IDF** 

Recall that: in [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)  (term frequency â€“ inverse document frequency)

  * TF  = frequency of term
  * IDF = logarithm of inverse of the frequency with which documents have that term
  

More formally, for a given document $d$ and term $t$, the **Term Frequency** is the number of times term $t$ appears in document $d$ :


$$
TF(d, t) = \text{# times }t \text{ appears in document }d
$$

To account for terms that appear frequently in the domain of interest, we compute the **Inverse Document Frequency** of term $t$, calculated over the entire corpus and defined as

$$
IDF(t) = \ln\left( \frac{\text{total number of documents}}{\text{# documents containing term } t} \right)
$$


- TF-IDF is high where a rare term is present or frequent in a document
- TF-IDF is near zero where a term is absent from a document, or abundant across all documents

We can use the `tidytext::bind_tf_idf()` function to easily compute TFIDF for every term:


```{r}
rei_tf_idf <- text_rei %>%
  unnest_tokens(word, comments) %>% # remove stop words
  anti_join(stop_words) %>% # remove stop words
  count(due_date, word, sort = TRUE) %>%
  bind_tf_idf(word, due_date, n) %>%
  arrange(-tf_idf) %>%
  group_by(due_date) %>%
  top_n(20) %>%
  ungroup()
```

Explore:

```{r}
rei_tf_idf
```


Create a visualization showing the top TF-IDF scores in the text data for this exercise:

```{r}
text_plot <- rei_tf_idf %>% 
  mutate(word = reorder(word, tf_idf)) %>% 
  filter(str_detect(due_date, c("01", "02"))) %>% 
  ggplot(aes(x = word, y = tf_idf, fill = due_date)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ due_date, scales = "free", ncol = 2) +
  coord_flip() +
  labs(x = "", "tf-idf", title = "Highest tf-idf words in text data") + 
  theme_minimal()
  
text_plot
```


# Topic Modeling


We will use Latent Dirichlet Allocation (LDA) for topic modeling, using the `topicmodels` package.


Right now our data frame is in a tidy form, with **one-term-per-document-per-row**, but the `topicmodels` package requires a `DocumentTermMatrix`. We can _cast_ a one-token-per-row table into a `DocumentTermMatrix` with `tidytext`'s function `cast_dtm()`. Additionally we remove any stop words.


```{r}
text_tidy <- text_rei %>%
  unnest_tokens(word, comments) %>% 
  anti_join(stop_words)
```


Check: 

```{r}
text_tidy
```

Use `cast_dtm()`: 

```{r}
text_dtm <- text_tidy %>% 
  count(due_date, word, sort = TRUE) %>% 
  cast_dtm(due_date, word, n)
```

Apply LDA:

```{r}
text_lda <- LDA(text_dtm, k = 3, control = list(seed = 3513))
text_lda
```


Extract the `beta`s (word-topic probabilities)

```{r}
text_topics <- tidy(text_lda, matrix = "beta")
  
text_topics
```

Notice that this has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the _probability of that term being generated from that topic_.

We could use `dplyr`'s `top_n()` to find the top 10 terms within each topic:

```{r}
top_terms <- text_topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

top_terms
```

Create a chart showing the highest word probabilities for each topic:

```{r}
graph_topics <- top_terms %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% 
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  labs(x = "", y = expression(beta))
  
graph_topics
```


## Network of bigrams

Next, we create a bigram network. First, perform tokenization and remove stop-words:

```{r}
text_filtered <- text_rei %>% 
  unnest_tokens(bigram, comments, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)
  
```

We then find the most common combination of words:

```{r}
text_counts <- text_filtered %>% 
  count(word1, word2, sort = TRUE)
```

Print the most frequent bigrams:

```{r}
# original counts
text_counts %>% 
  filter(n > 2)
```

Load the packages needed for graph visualization:

```{r, message=FALSE, warning=FALSE}
library(igraph)
library(ggraph)
```

Build graph from data frame using the `graph_from_data_frame()` function:

```{r}
# filter for only relatively common combinations
bigram_graph <- text_counts %>% 
  filter(n > 2) %>% 
  graph_from_data_frame()

bigram_graph
```

Create plot:

```{r}
set.seed(217)

# plot graph
bigrams_network <- ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = arrow(length = unit(2, "mm"))) +
  geom_node_point(color = "#53316B", 
                  size = 2) +
  geom_node_text(aes(label = name), 
                 vjust = 1.5, hjust = 0.2, size = 3) + 
  theme_void()

bigrams_network
```

